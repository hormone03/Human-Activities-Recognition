{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras_preprocessing.text' has no attribute 'tokenizer_from_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d5e71f30329b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#from keras import backend as K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/datasets/imdb.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_remove_long_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/preprocessing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mhashing_trick\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashing_trick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mTokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokenizer_from_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_from_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras_preprocessing.text' has no attribute 'tokenizer_from_json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "#from keras import backend as K\n",
    "from keras import optimizers\n",
    "#K.set_image_dim_ordering('th')\n",
    "# setting up a random seed for reproducibility\n",
    "random_seed = 611\n",
    "np.random.seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dl_utils import plot_metrics, plot_confusion_matrix\n",
    "np.random.seed(42)\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Importing libraries\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, BatchNormalization\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LIST OF FETAURES and ACTIVITIES__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNALS = [\"body_acc_x\",\n",
    "           \"body_acc_y\",\n",
    "           \"body_acc_z\",\n",
    "           \"body_gyro_x\",\n",
    "           \"body_gyro_y\",\n",
    "           \"body_gyro_z\",\n",
    "           \"total_acc_x\",\n",
    "           \"total_acc_y\",\n",
    "           \"total_acc_z\"]\n",
    "\n",
    "ACTIVITIES = {1 : 'WALKING',\n",
    "              2 : 'WALKING_UPSTAIRS',\n",
    "              3 : 'WALKING_DOWNSTAIRS',\n",
    "              4 : 'SITTING',\n",
    "              5 : 'STANDING',\n",
    "              6 : 'LAYING'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Loading the data\n",
    "\n",
    "* Here we load the raw time-series data in which each datapoint has 128 readings each for 9 different signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def read_txt(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "def load_data(subset, signals=SIGNALS):\n",
    "    signals_data = []\n",
    "    \n",
    "    #signal_data\n",
    "    for s in signals:\n",
    "        #print(s)\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{s}_{subset}.txt'\n",
    "        signals_data.append(read_txt(filename).values)\n",
    "        \n",
    "    #label_data\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = read_txt(filename)[0]\n",
    "    \n",
    "    #reshape the signals_data into (#samples, 128readings along time, 9featueres)\n",
    "    return np.transpose(signals_data, axes=(1, 2, 0)), pd.get_dummies(y).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain = load_data('train')\n",
    "Xtest, Ytest = load_data('test')\n",
    "\n",
    "print(f\"Shape of Xtrain : {Xtrain.shape}\")\n",
    "print(f\"Shape of Xtest  : {Xtest.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of data_points in Xtrain               : {Xtrain.shape[0]}\")\n",
    "print(f\"Number of data_points in Xtest                : {Xtest.shape[0]}\")\n",
    "\n",
    "print(f\"Number of features/signals                    : {Xtrain.shape[-1]}\")\n",
    "print(f\"Number of readings along time per each signal : {Xtrain.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(Xtrain, Ytrain, Xtest, Ytest, model):\n",
    "    \n",
    "    train = model.evaluate(Xtrain, Ytrain, verbose=0)\n",
    "    test = model.evaluate(Xtest, Ytest, verbose=0)\n",
    "    \n",
    "    display(pd.DataFrame([train, test], columns=['LOG_LOSS', 'ACCURACY'], index=['TRAIN', 'TEST']))\n",
    "    \n",
    "    return train + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_eval_dict = {'columns' : ['TRAIN_LOG_LOSS', 'TRAIN_ACC', \"TEST_LOG_LOSS\", \"TEST_ACC\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Neural Net\n",
    "\n",
    "* We'll use a __simple neural net with two hidden layers__ to train the model.\n",
    "* The input will be the flattened 128 vectors of 9 signals stripping the time dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_reshaped = Xtrain.reshape(Xtrain.shape[0], -1)\n",
    "Xtest_reshaped = Xtest.reshape(Xtest.shape[0], -1)\n",
    "\n",
    "print(f\"Shape of Xtrain_reshaped : {Xtrain_reshaped.shape}\")\n",
    "print(f\"Shape of Xtest_reshaped  : {Xtest_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(128, activation='relu', kernel_initializer='he_normal', \n",
    "                 input_shape=(Xtrain.reshape(Xtrain.shape[0], -1).shape[1:])))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(32, activation='relu', kernel_initializer='he_normal'))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(Ytrain.shape[1], activation='softmax'))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#checkpoint the best model based on log_loss\n",
    "filepath = 'saved_models/dnn_best_weights2.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model1.fit(Xtrain_reshaped,\n",
    "           Ytrain,\n",
    "           batch_size=512,\n",
    "           validation_data=(Xtest_reshaped, Ytest),\n",
    "           epochs=75, \n",
    "           shuffle=True,\n",
    "           callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(model1.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_eval_dict['DNN'] = metrics(Xtrain_reshaped, Ytrain, Xtest_reshaped, Ytest, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Ytest.argmax(1), model1.predict(Xtest_reshaped).argmax(1), labels=ACTIVITIES.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<font color='green'> OBSERVATIONS</font>__\n",
    "\n",
    "* A simple neural net achieved good performance ~90% accuracy.\n",
    "* The confusion matrix shows that the same issue exists as before <br>\n",
    "with the classifier having a hard time to seperate STANDING and SITTING classes.\n",
    "* The precision and recall values of classes : SITTING and STANDING are worse.\n",
    "* The log loss(0.26) is significantly greater than the linear model's one(0.16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 LSTM\n",
    "\n",
    "* We'll build an RNN with two LSTM layers stacked upon one other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliazing the sequential model\n",
    "model2 = Sequential()\n",
    "# Configuring the parameters\n",
    "model2.add(LSTM(96, input_shape=(Xtrain.shape[1:]), return_sequences=True))\n",
    "# Adding a dropout layer\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "model2.add(LSTM(64))\n",
    "# Adding a dropout layer\n",
    "model2.add(Dropout(0.7))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model2.add(Dense(Ytrain.shape[1], activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\n",
    "# Compiling the model\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "filepath = 'saved_models/lstm_best_weights.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model2.fit(Xtrain,\n",
    "          Ytrain,\n",
    "          batch_size=1024,\n",
    "          validation_data=(Xtest, Ytest),\n",
    "          epochs=100,\n",
    "          shuffle=True,\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(model2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'saved_models/lstm_best_weights.hdf5'\n",
    "model2.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_eval_dict['LSTM'] = metrics(Xtrain, Ytrain, Xtest, Ytest, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Ytest.argmax(1), model2.predict(Xtest).argmax(1), labels=ACTIVITIES.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<font color='green'> OBSERVATIONS</font>__\n",
    "\n",
    "* Training a complex network like RNN requires huge computational resources and data.\n",
    "* The same issue exists here. The performance is worser than the linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Models Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'columns' in models_eval_dict:\n",
    "    columns = models_eval_dict.pop('columns')\n",
    "    \n",
    "display(pd.DataFrame.from_dict(models_eval_dict, orient='index', columns=columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Both the deep neural nets (DNN and RNN) failed to overcome the limitations of the linear models.\n",
    "* __It is to be noted that the models were trained on the raw time-series data__.\n",
    "* Even a simple 1 layer neural net tends to perform on par with these models.\n",
    "* Both the models showed signs of overfitting.\n",
    "* After much  hyper-parameter tuning, it was found out that __we are beating a dead horse__ \n",
    "<br> and the performance never seemed to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
