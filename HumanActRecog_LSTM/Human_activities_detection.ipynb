{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "plt.style.use('ggplot')\n",
    "file_path='/Users/hormone03/Desktop/HumanActRecog_LSTM/dataset/WISDM_ar_v1.1_raw.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    column_names = ['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "    data = pd.read_csv(file_path,header = None, names = column_names, comment=';')\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(read_data(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis = 0)\n",
    "    sigma = np.std(dataset,axis = 0)\n",
    "    return (dataset - mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_axis(ax, x, y, title):\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
    "    ax.set_xlim([min(x), max(x)])\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activity(activity,data):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize = (15, 10), sharex = True)\n",
    "    plot_axis(ax0, data['timestamp'], data['x-axis'], 'x-axis')\n",
    "    plot_axis(ax1, data['timestamp'], data['y-axis'], 'y-axis')\n",
    "    plot_axis(ax2, data['timestamp'], data['z-axis'], 'z-axis')\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig.suptitle(activity)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_signal(data,window_size = 90):\n",
    "    segments = np.empty((0,window_size,3))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data['timestamp'], window_size):\n",
    "        x = data[\"x-axis\"][start:end]\n",
    "        y = data[\"y-axis\"][start:end]\n",
    "        z = data[\"z-axis\"][start:end]\n",
    "        if(len(dataset['timestamp'][start:end]) == window_size):\n",
    "            segments = np.vstack([segments,np.dstack([x,y,z])])\n",
    "            labels = np.append(labels,stats.mode(data[\"activity\"][start:end])[0][0])\n",
    "    return segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_data(file_path)\n",
    "dataset.dropna(axis=0, how='any', inplace= True)\n",
    "dataset.drop_duplicates(['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis'], keep= 'first', inplace= True)\n",
    "dataset['x-axis'] = feature_normalize(dataset['x-axis'])\n",
    "dataset['y-axis'] = feature_normalize(dataset['y-axis'])\n",
    "dataset['z-axis'] = feature_normalize(dataset['z-axis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for activity in np.unique(dataset[\"activity\"]):\n",
    "    subset = dataset[dataset[\"activity\"] == activity][:180]\n",
    "    plot_activity(activity,subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, labels = segment_signal(dataset)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "reshaped_segments = segments.reshape(len(segments), 1,90, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "train_x = reshaped_segments[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = reshaped_segments[~train_test_split]\n",
    "test_y = labels[~train_test_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 6\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 1000\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 30\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "#print(\"Formatted Number: \"+\"{:.2f}\".format(x));\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                acc = sess.run(accuracy, feed_dict={X: train_x, Y: train_y})\n",
    "                test_losses.append(c)\n",
    "                test_accuracies.append(acc)\n",
    "            \n",
    "                t_acc = sess.run(accuracy, feed_dict={X: test_x, Y: test_y})\n",
    "                _, t_loss = sess.run([train_op, loss], feed_dict={X: test_x, Y: test_y})\n",
    "                print(\"Epoch: \", epoch+1, \"|Train loss = \", c,\n",
    "                      \"|Train acc = \", acc, \" |Test loss = \", t_loss, \"|Test acc = \", t_acc)\n",
    "                train_accuracies.append(t_acc)\n",
    "                train_losses.append(t_loss)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(\" After Epoch: \", epoch+1,\n",
    "              \" |Test acc = \", sess.run(accuracy, feed_dict={X: test_x, Y: test_y}), \" ###\")\n",
    "        pred_y = sess.run(tf.argmax(y_, 1), feed_dict={X: test_x})\n",
    "        cm = metrics.confusion_matrix(np.argmax(test_y, 1), pred_y,)\n",
    "        print(cm, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
